{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rPXnHd313o9",
        "outputId": "2a366432-0513-4fd8-a030-9dce50aa3fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Step 1: Loading the NLP model and extracting three-dimensional vectors\n",
            "Total words:  243\n",
            "All words in the model:  ['country', 'city', 'China', 'Iraq', 'oil', 'town', 'Canada', 'London', 'England', 'Australia', 'Japan', 'Pakistan', 'Iran', 'gas', 'happy', 'Russia', 'Afghanistan', 'France', 'Germany', 'Georgia', 'Baghdad', 'village', 'Spain', 'Italy', 'Beijing', 'Jordan', 'Paris', 'Ireland', 'Turkey', 'Egypt', 'Lebanon', 'Taiwan', 'Tokyo', 'Nigeria', 'Vietnam', 'Moscow', 'Greece', 'Indonesia', 'sad', 'Syria', 'Thailand', 'Libya', 'Zimbabwe', 'Cuba', 'Ottawa', 'Tehran', 'Sudan', 'Kenya', 'Philippines', 'Sweden', 'Poland', 'Ukraine', 'Rome', 'Venezuela', 'Switzerland', 'Berlin', 'Bangladesh', 'Portugal', 'Ghana', 'Athens', 'king', 'Madrid', 'Somalia', 'Dublin', 'Qatar', 'Chile', 'Islamabad', 'Bahrain', 'Nepal', 'Norway', 'Serbia', 'Kabul', 'continent', 'Brussels', 'Belgium', 'Uganda', 'petroleum', 'Cairo', 'Denmark', 'Austria', 'Jamaica', 'Georgetown', 'Bangkok', 'Finland', 'Peru', 'Romania', 'Bulgaria', 'Hungary', 'Vienna', 'Kingston', 'Manila', 'Cyprus', 'Azerbaijan', 'Copenhagen', 'Fiji', 'Tunisia', 'Kazakhstan', 'queen', 'Beirut', 'Jakarta', 'Croatia', 'Belarus', 'Algeria', 'Malta', 'Morocco', 'Rwanda', 'Bahamas', 'Damascus', 'Ecuador', 'Angola', 'Canberra', 'Liberia', 'Honduras', 'Tripoli', 'Slovakia', 'Doha', 'Armenia', 'Taipei', 'Oman', 'Nairobi', 'Santiago', 'Guinea', 'Uruguay', 'Stockholm', 'Slovenia', 'Zambia', 'Havana', 'Uzbekistan', 'Belgrade', 'Mogadishu', 'Khartoum', 'Botswana', 'Kyrgyzstan', 'Dhaka', 'Namibia', 'Ankara', 'Abuja', 'Lima', 'Harare', 'Warsaw', 'Malawi', 'Lisbon', 'Latvia', 'Niger', 'Lithuania', 'Estonia', 'Samoa', 'Oslo', 'Nicaragua', 'Hanoi', 'Sofia', 'Macedonia', 'Senegal', 'Mozambique', 'Guyana', 'Mali', 'Accra', 'Kathmandu', 'Tbilisi', 'Helsinki', 'Montenegro', 'Caracas', 'Laos', 'Budapest', 'Kiev', 'Turkmenistan', 'Eritrea', 'Albania', 'Madagascar', 'Nassau', 'Kampala', 'Amman', 'Greenland', 'Belize', 'Moldova', 'Burundi', 'Tajikistan', 'Baku', 'Astana', 'Gambia', 'Bucharest', 'joyful', 'Monrovia', 'Mauritania', 'Algiers', 'Muscat', 'Bern', 'Luanda', 'Dakar', 'Tunis', 'Gabon', 'Minsk', 'Liechtenstein', 'Suva', 'Yerevan', 'Zagreb', 'Bishkek', 'Manama', 'Kigali', 'Riga', 'Lusaka', 'Tashkent', 'Nicosia', 'Valletta', 'Windhoek', 'Dominica', 'Quito', 'Tallinn', 'Bratislava', 'Tegucigalpa', 'Skopje', 'Gaborone', 'Rabat', 'Maputo', 'Suriname', 'Vilnius', 'Montevideo', 'Ljubljana', 'Tirana', 'Dushanbe', 'Ashgabat', 'Asmara', 'Tuvalu', 'Managua', 'Conakry', 'Banjul', 'Bamako', 'Lilongwe', 'Vientiane', 'Chisinau', 'Roseau', 'Nouakchott', 'Podgorica', 'Niamey', 'Bujumbura', 'Apia', 'Antananarivo', 'Libreville', 'Belmopan', 'Vaduz', 'Paramaribo', 'Nuuk', 'Funafuti']\n",
            "Step 1 completed: DataFrame created with three-dimensional vectors.\n",
            "\n",
            "-------------------- 1 --------------------\n",
            "\n",
            "Words in the dataset:\n",
            " 0    country\n",
            "1       city\n",
            "2      China\n",
            "3       Iraq\n",
            "4        oil\n",
            "Name: word, dtype: object\n",
            "\n",
            "-------------------- 2 --------------------\n",
            "\n",
            "      word  vector_x  vector_y  vector_z\n",
            "0  country  0.746037 -0.387964 -0.482691\n",
            "1     city  0.102494  0.140384 -1.189890\n",
            "2    China  0.831055 -0.129500 -0.312599\n",
            "3     Iraq  0.656337 -0.177791 -0.338381\n",
            "4      oil  0.658345 -0.432464 -0.621213\n",
            "Step 2: Finding the closest word to a sample vector\n",
            "The closest word to the vector [0.5, -0.2, 0.3]: Syria\n",
            "Conclusion: The word \"Syria\" has the strongest semantic\n",
            "connection to the given vector, demonstrating the proximity of their values in the\n",
            "vector space.\n",
            "\n",
            "-------------------- 3 --------------------\n",
            "\n",
            "Step 3: Finding an orthogonal word for two sample words\n",
            "Orthogonal word to 'city' and 'China': Fiji\n",
            "\n",
            "Conclusions:\n",
            "\n",
            "The result \"Orthogonal word to 'city' and 'China': Fiji\" means that the word\n",
            "\"Fiji\" is closest to the vector obtained as a result of the cross product of the vectors\n",
            "of the words \"city\" and \"China\".\n",
            "\n",
            "Conclusion:\n",
            "Semantic connection: The cross product represents a new vector that is orthogonal\n",
            "to the original two. This allows us to identify concepts or words that do not have a direct\n",
            "semantic connection with \"city\" and \"China\" but may be important in another context.\n",
            "\n",
            "Fiji as a result: The vector \"Fiji\" indicates the uniqueness of this word compared to\n",
            "the original ones (\"city\" and \"China\"). This may mean that \"Fiji\" represents a distinct semantic\n",
            "category (e.g., a geographical entity) but is unique in a context\n",
            "that does not overlap with the other two.\n",
            "\n",
            "Practical significance:\n",
            "The cross product helps find semantically independent (orthogonal) words,\n",
            "which can be useful in tasks of identifying unique concepts or categories in\n",
            "the word vector space.\n",
            "\n",
            "\n",
            "-------------------- 4 --------------------\n",
            "\n",
            "Step 4: Calculating the angle between two sample words\n",
            "The angle between 'city' and 'China': 66.00 degrees\n",
            "Conclusions:\n",
            "An angle of 66 degrees between the vectors of the words 'city' and 'China' indicates\n",
            "their semantic relationship in the vector space. This means that the words have some\n",
            "similarity in meaning but are not very close or identical.\n",
            "\n",
            "Summary:\n",
            "\n",
            "Large angle (close to 90°): indicates a weak semantic relationship or orthogonality of meanings.\n",
            "Small angle (close to 0°): indicates strong similarity or near identity of meanings.\n",
            "In this case, 'city' and 'China' are related concepts (both belong to the geographical context),\n",
            "but they are not as similar as, for example, 'city' and 'town.' This demonstrates how\n",
            "vector representations of words allow us to evaluate the relative similarity between concepts.\n",
            "\n",
            "\n",
            "-------------------- Conclusions --------------------\n",
            "\n",
            "\n",
            "1. The NLP model was loaded, and a three-dimensional representation of words was created for analysis.\n",
            "2. A function was implemented to find the nearest word to a given vector, demonstrating work with semantic similarities.\n",
            "3. A vector cross-product was used to find an orthogonal word, allowing for the identification of unique relationships between words.\n",
            "4. A function for calculating the angle between word vectors was implemented, helping analyze the semantic difference between them.\n",
            "\n",
            "These methods show how semantic relationships between words can be interpreted in a vector space, which is useful for natural language processing (NLP) tasks.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.linalg import norm\n",
        "import pickle\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Connecting Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Loading the NLP model\n",
        "print(\"Step 1: Loading the NLP model and extracting three-dimensional vectors\")\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/Numerical Python/word_embeddings_subset.p'\n",
        "with open(file_path, 'rb') as f:\n",
        "    word_embeddings = pickle.load(f)\n",
        "\n",
        "print(\"Total words: \", len(word_embeddings))\n",
        "\n",
        "# Extracting words and vectors\n",
        "words = list(word_embeddings.keys())\n",
        "print(\"All words in the model: \", words)\n",
        "vectors = np.array([word_embeddings[word] for word in words])\n",
        "\n",
        "# Reducing vectors to 3 dimensions using PCA\n",
        "pca = PCA(n_components=3)\n",
        "vectors_3d = pca.fit_transform(vectors)\n",
        "\n",
        "# Creating a DataFrame with three-dimensional vectors\n",
        "df = pd.DataFrame({\n",
        "    'word': words,\n",
        "    'vector_x': vectors_3d[:, 0],\n",
        "    'vector_y': vectors_3d[:, 1],\n",
        "    'vector_z': vectors_3d[:, 2]\n",
        "})\n",
        "print(\"Step 1 completed: DataFrame created with three-dimensional vectors.\\n\")\n",
        "\n",
        "# Checking the result\n",
        "print(f\"{'-' * 20} 1 {'-' * 20}\\n\")\n",
        "print(\"Words in the dataset:\\n\", df['word'].head())\n",
        "print(f\"\\n{'-' * 20} 2 {'-' * 20}\\n\")\n",
        "print(df.head())\n",
        "\n",
        "\n",
        "# Step 2: Function to find the closest word to a given three-dimensional vector\n",
        "def find_closest_word(vector, df):\n",
        "    df['distance'] = np.sqrt((df['vector_x'] - vector[0])**2 +\n",
        "                             (df['vector_y'] - vector[1])**2 +\n",
        "                             (df['vector_z'] - vector[2])**2)\n",
        "    closest_word = df.loc[df['distance'].idxmin()]['word']\n",
        "    return closest_word\n",
        "\n",
        "# Testing the function with a specific vector\n",
        "print(\"Step 2: Finding the closest word to a sample vector\")\n",
        "example_vector = [0.5, -0.2, 0.3]\n",
        "closest_word_example = find_closest_word(example_vector, df)\n",
        "print(f\"The closest word to the vector {example_vector}: {closest_word_example}\")\n",
        "print(f\"Conclusion: The word \\\"{closest_word_example}\\\" has the strongest semantic\\n\"\n",
        "f\"connection to the given vector, demonstrating the proximity of their values in the\\n\"\n",
        "f\"vector space.\")\n",
        "print(f\"\\n{'-' * 20} 3 {'-' * 20}\\n\")\n",
        "\n",
        "\n",
        "# Step 3: Calculating the cross product to find an orthogonal word\n",
        "def find_orthogonal_word(vector1, vector2, df):\n",
        "    cross_product = np.cross(vector1, vector2)\n",
        "    return find_closest_word(cross_product, df)\n",
        "\n",
        "# Example usage with two arbitrary words\n",
        "print(\"Step 3: Finding an orthogonal word for two sample words\")\n",
        "word1, word2 = 'city', 'China'\n",
        "vector1 = df.loc[df['word'] == word1, ['vector_x', 'vector_y', 'vector_z']].values[0]\n",
        "vector2 = df.loc[df['word'] == word2, ['vector_x', 'vector_y', 'vector_z']].values[0]\n",
        "orthogonal_word = find_orthogonal_word(vector1, vector2, df)\n",
        "print(f\"Orthogonal word to '{word1}' and '{word2}': {orthogonal_word}\")\n",
        "print(\"\"\"\n",
        "Conclusions:\n",
        "\n",
        "The result \"Orthogonal word to 'city' and 'China': Fiji\" means that the word\n",
        "\"Fiji\" is closest to the vector obtained as a result of the cross product of the vectors\n",
        "of the words \"city\" and \"China\".\n",
        "\n",
        "Conclusion:\n",
        "Semantic connection: The cross product represents a new vector that is orthogonal\n",
        "to the original two. This allows us to identify concepts or words that do not have a direct\n",
        "semantic connection with \"city\" and \"China\" but may be important in another context.\n",
        "\n",
        "Fiji as a result: The vector \"Fiji\" indicates the uniqueness of this word compared to\n",
        "the original ones (\"city\" and \"China\"). This may mean that \"Fiji\" represents a distinct semantic\n",
        "category (e.g., a geographical entity) but is unique in a context\n",
        "that does not overlap with the other two.\n",
        "\n",
        "Practical significance:\n",
        "The cross product helps find semantically independent (orthogonal) words,\n",
        "which can be useful in tasks of identifying unique concepts or categories in\n",
        "the word vector space.\n",
        "\"\"\")\n",
        "print(f\"\\n{'-' * 20} 4 {'-' * 20}\\n\")\n",
        "\n",
        "\n",
        "# Step 4: Function to calculate the angle between two vectors\n",
        "def calculate_angle(vector1, vector2):\n",
        "    dot_product = np.dot(vector1, vector2)\n",
        "    magnitude_product = norm(vector1) * norm(vector2)\n",
        "    cosine_angle = dot_product / magnitude_product\n",
        "    # Clipping to avoid numerical errors\n",
        "    angle = np.arccos(np.clip(cosine_angle, -1.0, 1.0))\n",
        "    return np.degrees(angle)\n",
        "\n",
        "# Testing the function with two words\n",
        "print(\"Step 4: Calculating the angle between two sample words\")\n",
        "angle_between_words = calculate_angle(vector1, vector2)\n",
        "print(f\"The angle between '{word1}' and '{word2}': {angle_between_words:.2f} degrees\")\n",
        "\n",
        "print(\"\"\"Conclusions:\n",
        "An angle of 66 degrees between the vectors of the words 'city' and 'China' indicates\n",
        "their semantic relationship in the vector space. This means that the words have some\n",
        "similarity in meaning but are not very close or identical.\n",
        "\n",
        "Summary:\n",
        "\n",
        "Large angle (close to 90°): indicates a weak semantic relationship or orthogonality of meanings.\n",
        "Small angle (close to 0°): indicates strong similarity or near identity of meanings.\n",
        "In this case, 'city' and 'China' are related concepts (both belong to the geographical context),\n",
        "but they are not as similar as, for example, 'city' and 'town.' This demonstrates how\n",
        "vector representations of words allow us to evaluate the relative similarity between concepts.\n",
        "\"\"\")\n",
        "\n",
        "print(f\"\\n{'-' * 20} Conclusions {'-' * 20}\\n\")\n",
        "# Conclusions\n",
        "print(\"\"\"\n",
        "1. The NLP model was loaded, and a three-dimensional representation of words was created for analysis.\n",
        "2. A function was implemented to find the nearest word to a given vector, demonstrating work with semantic similarities.\n",
        "3. A vector cross-product was used to find an orthogonal word, allowing for the identification of unique relationships between words.\n",
        "4. A function for calculating the angle between word vectors was implemented, helping analyze the semantic difference between them.\n",
        "\n",
        "These methods show how semantic relationships between words can be interpreted in a vector space, which is useful for natural language processing (NLP) tasks.\n",
        "\"\"\")\n"
      ]
    }
  ]
}